# Day14

## 8.1 加深学习网络

### 8.1.1 向更深的网络出发

构建一个进行MNIST手写数字识别的深度CNN：

**(Conv -> ReLU -> Conv -> ReLU -> Pool) x3 -> (Affine -> ReLU -> Dropout) -> (Affine -> Dropout -> Softmax)**

这个网络有如下特点：

- 基于3*3小型滤波器的卷积层
- 激活函数是ReLU
- 全链接层的后面使用Dropout层
- 基于Adam的最优化
- 使用He的初始值作为权重初始值

### 8.1.2 进一步提高识别精度

- 集成学习
- 学习率衰减
- **Data Augmentation**

### 8.1.3 加深层的动机

*与没有加深层的网络相比，加深层的网络可以用更少的参数达到同等水平（甚至更强的）表现力*

## 8.2 深度学习的小历史

### 8.2.1 ImageNet

这是一个拥有超过100w+图片的超级数据集，并且每张图像都关联了标签（类别名）常用于比赛。

### 8.2.2 VGG

VGG是卷积层和池化层构成的基础CNN，结构简单，应用性强

### 8.2.3 GoogLeNet

在横向上有“宽度”，称为“Inception”结构。

### 8.2.4 ResNet

设计了**shortcut**，横跨输入数据的卷积层，解决了过度加深层数使得学习不能顺利进行的问题。

## 8.3 深度学习的高速化

### 8.3.1 需要努力解决的问题

大部分时间消耗在卷积层上，因此，主要解决的问题就是如何高速、高效地进行大量的乘积累加运算。

### 8.3.2 基于GPU的高速化

GPU可以高速地进行并行数值计算
深度学习的框架中使用了NVIDIA提供的CUDA开发环境，cuDNN是在CUDA上运行的库

### 8.3.3 分布式学习

Google的**TensorFlow**，微软的**CNTK**支持分布式学习，结合大数据中心的低延迟高吞吐网络为支撑，可以达到惊人的效果。

### 8.3.4 运算精度的位数缩减

得益于神经网络的健壮性，即便是16位的半精度浮点数也能顺利进行学习，今后半精度浮点数也将作为标准使用。

## 8.4 深度学习的应用案例

### 8.4.1 物体检测
### 8.4.2 图像分割
### 8.4.3 图像标题的生成

## 8.5 深度学习的未来

### 8.5.1 图像风格变换
### 8.5.2 图像的生成
### 8.5.3 自动驾驶
### 8.5.4 强化学习

